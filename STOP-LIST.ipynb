{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Option 4 – Stop-List Builder (Explanation)"
      ],
      "metadata": {
        "id": "ENDW8BzZg1Km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project implements Option 4 of the assignment , creating a stop-list from multiple English Wikipedia pages.\n",
        "The goal is to find the most frequent words (common across many documents) and build a list of words to ignore in future text processing tasks."
      ],
      "metadata": {
        "id": "7ScE3PmPg6Ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "downloading every liberary that is needed"
      ],
      "metadata": {
        "id": "7Bna3AmUhCE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Option 4: Stop-List Builder ---\n",
        "!pip install nltk wikipedia unidecode -q\n",
        "\n",
        "import nltk, re, string, collections, os, math\n",
        "import wikipedia\n",
        "from nltk.corpus import stopwords\n",
        "from unidecode import unidecode\n"
      ],
      "metadata": {
        "id": "KOvI3ARGf2ME"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords', quiet=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qrHA4vFf-Ms",
        "outputId": "dc84467f-459d-472c-b8ec-120ae70846a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1 – Choosing 10 Wikipedia Pages\n"
      ],
      "metadata": {
        "id": "cyyaKbJAgBjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We selected 10 Wikipedia articles in English related to computer science and AI.\n",
        "Each page will be downloaded and analyzed separately."
      ],
      "metadata": {
        "id": "npUjjAABhZDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAGES = [\n",
        "    \"Artificial intelligence\",\n",
        "    \"Supervised learning\",\n",
        "    \"Cluster analysis\",\n",
        "    \"Natural language processing\",\n",
        "    \"Deep learning\",\n",
        "    \"Science and technology studies\",\n",
        "    \"Bibliometrics\",\n",
        "    \"Speech recognition\",\n",
        "    \"Artificial neural network\",\n",
        "    \"Reinforcement learning\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "wbZsaAV4gB-c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2 – Downloading and Cleaning the Text"
      ],
      "metadata": {
        "id": "zujrUXYhh4D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uses the wikipedia library to fetch the full text of each article.\n",
        "\n",
        "Converts all text to lowercase.\n",
        "\n",
        "Removes punctuation, numbers, and non-English characters using regular expressions (re.sub).\n",
        "\n",
        "Keeps only letters and spaces for clean tokenization later."
      ],
      "metadata": {
        "id": "6qJaHo_ygE_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2) downloading the texts from wikipedia\n",
        "\n",
        "#content = wikipedia.page(page, auto_suggest=True, redirect=True).content\n",
        "wikipedia.set_lang(\"en\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = unidecode(text.lower())\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    return text\n",
        "\n",
        "docs = {}\n",
        "for page in PAGES:\n",
        "    try:\n",
        "        content = wikipedia.page(page).content\n",
        "        docs[page] = clean_text(content)\n",
        "        print(\"Downloaded:\", page)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", page, e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZmgFjESgFZp",
        "outputId": "3a392a4d-865f-4c48-d67c-bd6a91c33002"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: Artificial intelligence\n",
            "Downloaded: Supervised learning\n",
            "Downloaded: Cluster analysis\n",
            "Downloaded: Natural language processing\n",
            "Downloaded: Deep learning\n",
            "Downloaded: Science and technology studies\n",
            "Downloaded: Bibliometrics\n",
            "Downloaded: Speech recognition\n",
            "Downloaded: Artificial neural network\n",
            "Downloaded: Reinforcement learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3 – Splitting Each Document into 10 Parts"
      ],
      "metadata": {
        "id": "-u7R5ebdgP-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each article is divided into 10 equal chunks to simulate smaller sub-documents.\n",
        "In total, we get around 100 text parts (10 articles × 10 parts each)."
      ],
      "metadata": {
        "id": "J1Hk_-FZh-Y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3) we split the texts to 10 chunks\n",
        "def split_chunks(text, n=10):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    size = max(1, len(words)//n)\n",
        "    return [\" \".join(words[i*size:(i+1)*size]) for i in range(n)]\n",
        "\n",
        "chunks = {}\n",
        "for name, text in docs.items():\n",
        "    cks = split_chunks(text)\n",
        "    for i, c in enumerate(cks):\n",
        "        chunks[f\"{name}_part{i+1}\"] = c\n",
        "\n",
        "print(f\"\\nTotal chunks created: {len(chunks)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMccVuxCgQcl",
        "outputId": "14c48ff7-0f09-40da-bcce-da4bf5ef3803"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total chunks created: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###Step 4 – Base Stopwords from NLTK\n"
      ],
      "metadata": {
        "id": "Qqd3lc1vgXUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load a predefined list of common English stop-words (like the, is, and, of, to).\n",
        "These are words that usually do not add meaning to text analysis and can be filtered out later."
      ],
      "metadata": {
        "id": "JghAvZ7yiI5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4) we take the stop words that were defined in the nltk.corpus library\n",
        "stop_words = set(stopwords.words(\"english\"))\n"
      ],
      "metadata": {
        "id": "eBmpqbCzgXtv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 5 – Counting Word Frequencies"
      ],
      "metadata": {
        "id": "w2Qf05y9iZ5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizes every chunk into individual words.\n",
        "\n",
        "Keeps only alphabetic tokens.\n",
        "\n",
        "Uses Counter to count how many times each word appears in all texts combined."
      ],
      "metadata": {
        "id": "BhzOAeDOif0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5)we count how many times the tokens appeared in the texts\n",
        "all_tokens = []\n",
        "for part_text in chunks.values():\n",
        "    tokens = nltk.word_tokenize(part_text)\n",
        "    tokens = [t for t in tokens if t.isalpha()]\n",
        "    all_tokens.extend(tokens)\n",
        "\n",
        "freq = collections.Counter(all_tokens)\n"
      ],
      "metadata": {
        "id": "gpDxNzaGga5L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 6 – Building the Custom Stop-List"
      ],
      "metadata": {
        "id": "wdHBRlgU7nBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After counting frequencies, we take the 50 most common words and create a custom stop-list.\n",
        "These represent the most repetitive, least informative words in the dataset."
      ],
      "metadata": {
        "id": "3qnJV64mgdPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6) we found the most common 50 stop words and print them\n",
        "TOP_N = 50\n",
        "custom_stoplist = [w for w, f in freq.most_common(TOP_N)]\n",
        "\n",
        "print(\"\\n=== Custom Stop-List (Top 50 words) ===\")\n",
        "for i, w in enumerate(custom_stoplist, 1):\n",
        "    print(f\"{i:2d}. {w}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHl84Jm4gdi_",
        "outputId": "4b12cca1-4e11-4942-c918-734e2b870446"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Custom Stop-List (Top 50 words) ===\n",
            " 1. the\n",
            " 2. of\n",
            " 3. and\n",
            " 4. to\n",
            " 5. a\n",
            " 6. in\n",
            " 7. is\n",
            " 8. that\n",
            " 9. for\n",
            "10. as\n",
            "11. s\n",
            "12. by\n",
            "13. learning\n",
            "14. are\n",
            "15. with\n",
            "16. on\n",
            "17. be\n",
            "18. or\n",
            "19. can\n",
            "20. data\n",
            "21. it\n",
            "22. an\n",
            "23. from\n",
            "24. this\n",
            "25. ai\n",
            "26. such\n",
            "27. neural\n",
            "28. displaystyle\n",
            "29. was\n",
            "30. used\n",
            "31. deep\n",
            "32. have\n",
            "33. speech\n",
            "34. networks\n",
            "35. not\n",
            "36. which\n",
            "37. science\n",
            "38. based\n",
            "39. g\n",
            "40. recognition\n",
            "41. has\n",
            "42. more\n",
            "43. research\n",
            "44. network\n",
            "45. e\n",
            "46. been\n",
            "47. clustering\n",
            "48. methods\n",
            "49. other\n",
            "50. models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 7 – Saving the Stop-List to a File"
      ],
      "metadata": {
        "id": "8oR8a8QGgfiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final stop-list is saved into a text file called\n",
        "output/stop_list.txt — one word per line."
      ],
      "metadata": {
        "id": "_-RIgzx37LwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 7) שמירת stop-list לקובץ\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "with open(\"output/stop_list.txt\", \"w\") as f:\n",
        "    for w in custom_stoplist:\n",
        "        f.write(w + \"\\n\")\n",
        "\n",
        "print(\"\\nStop-list saved to: output/stop_list.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5ZucrKcgf0G",
        "outputId": "82921aa5-8525-4e68-b6fc-3a3ca7ad19a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stop-list saved to: output/stop_list.txt\n"
          ]
        }
      ]
    }
  ]
}